---
title: "Abalone Age Regression"
author: "Danyang Dai, Yehong Qiu"
date: "2024-12-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Pre-processing

## Load the Data

```{r}
data = read.table("abalone.txt", header=FALSE, sep = ",")
names(data) = c("Sex","Length","Diameter","Height","Whole_weight",
                "Shucked_weight","Viscera_weight","Shell_weight","Rings")
```
## Statistical Summaries 

```{r}
summary(data)
```
## Data Transformation

```{r}
# Convert Categorical Variables to Factors
data$Sex<- as.factor(data$Sex)

# Create Feature AGE as response data
data$Age = data$Rings + 1.5
```
## Explorative Data Analysis 

### Histogram Plot

```{r}
par(mfrow = c(2, 2))
for(i in 2:10) {
hist(data[, i], main=paste("Histogram of", names(data)[i]), xlab = paste(names(data)[i]))}
```
### Pie Chart

```{r}
### pie chart with percentages
n <- nrow(data)
lbls <- c('F','I','M')
pct <- round(100*table(data$Sex)/n)
lab <- paste(lbls,pct)
lab <- paste(lab,'%',sep='')

pie(table(data$Sex),labels=lab,col=c('blue','purple','green'),
main='Sex: pie chart with percentage')
```
### Draw Boxplot

```{r}
boxplot(data$Rings~data$Sex,main='Rings: side-by-side box plot by Sex',
        xlab='Sex',ylab='Rings',col=rainbow(3))
#boxplot(newdata$Rings~data$Sex,main='Rings: side-by-side box plot by Sex',
#        xlab='Sex',ylab='Rings',col=rainbow(3))
```
### Correlation Scatter Plot

```{r}
panel.cor = function(x, y, digits = 2, cex_cor = 1.2) {
  
  usr = par("usr")  
  on.exit(par(usr)) 
  par(usr = c(0, 1, 0, 1))
  
  cor_val = cor(x,y, use="complete.obs")
  txt = formatC(cor_val, format = "f", digits = digits)
  text(0.5, 0.5, paste(txt), cex = cex_cor)
}
pairs(data[,2:9], upper.panel=panel.cor)
#pairs(newdata[,2:9], lower.panel=panel.cor)
```
# Preliminary Fitting

## First-order Full Model

### Fit the Model.0

```{r}
dataS = data[,1:8]
dataS$Age = data$Age

model.0 = lm(Age~.,data=dataS)
```
### Model Summary

```{r}
summary(model.0)
anova(model.0)
```
### Residual Plots

```{r}
par(mfrow = c(2, 2))
plot(model.0, which=1:5)

par(mfrow = c(1, 1))
```
### Calculate SSEp,Square_Rp,Square_Rap,AICp,BICp, and Pressp (model.0)

```{r}
n=nrow(data)
p_full=length(model.0$coefficients)
sse_full=sum(model.0$residuals^2)
aic_full=n*log(sse_full/n)+2*p_full
bic_full = n*log(sse_full/n)+log(n)*p_full
PRESS_p_full=sum((model.0$residuals/(1-influence(model.0)$hat))^2)

p_full
sse_full
aic_full
bic_full
PRESS_p_full # not overfitting
```
## Transform through Box-Cox 
批注：可能还需要对其他特征进行转换
### Box-Cox test

```{r}
library(MASS)
boxcox(model.0)
```
### Ages Transformation

```{r}

par(mfrow=c(2,2))

hist(log(dataS$Age))
hist(sqrt(dataS$Age))
hist(1/dataS$Age)
hist((dataS$Age)^(-1/3))
par(mfrow=c(1,1))
```

### Weight Multicollinearity Test
```{r}
dataweight <- dataS[,5:8]
weight_fit = lm(Whole_weight~.,data=dataweight)

summary(weight_fit)
anova(weight_fit)
```

### Create New Data(Transformed Version)

```{r}
newdata = dataS
newdata$Age = (dataS$Age)^(-1/3)
newdata$Whole_weight = NULL
```

## First-order Full Model with Transformation

### Fit the Model.0.1
```{r}
model.0.1 = lm(Age~.,data=newdata)
summary(model.0.1)
anova(model.0.1)
```

### Residual Plots
```{r}
par(mfrow=c(2,2))
plot(model.0.1, which=1:5)

par(mfrow=c(1,1))
```
There are a few outliers or else the model is acceptable (residuals nonlinearity not obvious, normality okay, heteroscedasticity not obvious).

### Calculate SSEp,Square_Rp,Square_Rap,AICp,BICp, and Pressp (model.0.1)
```{r}
n=nrow(newdata)
p_full=length(model.0.1$coefficients)
sse_full=sum(model.0.1$residuals^2)
aic_full=n*log(sse_full/n)+2*p_full
bic_full = n*log(sse_full/n)+log(n)*p_full
PRESS_p_full=sum((model.0.1$residuals/(1-influence(model.0.1)$hat))^2)

p_full
sse_full
aic_full
bic_full
PRESS_p_full # not much larger than sse, not overfitting
```
```{r}
model.0.3 = lm(Age~Sex+Length+Diameter+Height+
                 Shucked_weight+Viscera_weight+Shell_weight+Whole_weight,data=newdata)
summary(model.0.3)
anova(model.0.3)

```
We should not delete `Whole_weight` directly even though from `weight_fit`,  because from here the general linear test shows that the coefficient of `Whole_weight` being non-zero is significant, and from AIC and BIC, SSE, PRESSp criterion.

```{r, model.0.2}
newdata$Whole_weight = NULL
model.0.2 = lm(Age~.,data=newdata)
summary(model.0.2)
anova(model.0.2)

par(mfrow=c(2,2))
plot(model.0.2, which=1:5)

par(mfrow=c(1,1))

n=nrow(newdata)
p_full=length(model.0.2$coefficients)
sse_full=sum(model.0.2$residuals^2)
aic_full=n*log(sse_full/n)+2*p_full
bic_full = n*log(sse_full/n)+log(n)*p_full
PRESS_p_full=sum((model.0.2$residuals/(1-influence(model.0.2)$hat))^2)

p_full
sse_full
aic_full
bic_full
PRESS_p_full # not much larger than sse, not overfitting

```

### Calculate VIF
```{r}
# VIF
library(car)
vif_values <- vif(model.0.2)

vif_values
```

strong multi-colinearity among X variables.


# Addressing Multi-colinearity

## Data Splitting
```{r}
set.seed(123)
idx = sample(1:nrow(newdata), size=0.7*nrow(newdata), replace=FALSE)
data_t = newdata[idx,]
data_v = newdata[-idx,]
```
## examine whether the training data and validation data look alike
```{r}
par(mfrow=c(2,2))
for (col_name in c('Length', 'Diameter', 'Height', 'Whole_weight', 'Shucked_weight', 'Viscera_weight', 'Shell_weight', 'Age')){
  boxplot(data_t[, col_name], data_v[, col_name], main = col_name, names = c('training data', 'validation data'))
}
par(mfrow=c(1,1))

n <- c(nrow(data_t), nrow(data_v))
lbls <- c('F','I','M')
pct_t <- round(100*table(data_t$Sex)/n[1])
pct_v <- round(100*table(data_v$Sex)/n[2])
lab_t <- paste(lbls,pct_t)
lab_v <- paste(lbls,pct_v)
lab_t <- paste(lab_t,'%',sep='')
lab_v <- paste(lab_v,'%',sep='')
par(mfrow=c(1,2))
pie(table(data_t$Sex),labels=lab_t,col=c('blue','purple','green'),
  main='training data')
pie(table(data_v$Sex),labels=lab_v,col=c('blue','purple','green'),
  main='validation data')
```
### Estimate RMSPE on ols (same form with model.0.1 but fitted on data_t)
```{r}
aba.ols = lm(Age~.,data=data_t)
Age.vali.ols = predict(aba.ols, data_v)

c(sqrt(mean((aba.ols$fitted.values - data_t$Age)^2)), sqrt(mean((Age.vali.ols - data_v$Age)^2)))
```

### Ridge regression

```{r}
model.0.1.ridge = lm.ridge(Age~., data=data_t, 
                  lambda = exp(seq(log(1e0),log(1e-2),length.out = 1000)))
lambda.GCV = model.0.1.ridge$lambda[which.min(model.0.1.ridge$GCV)]
plot(model.0.1.ridge$lambda, model.0.1.ridge$GCV, xlab = expression(lambda), ylab='GCV', type='l')
abline(v=lambda.GCV, lty=2)
```


```{r}
matplot(model.0.1.ridge$lambda, t(model.0.1.ridge$coef), type='l', lty=1, 
        xlab=expression(lambda), ylab=expression(hat(beta)))
abline(v=lambda.GCV, lty=2) ##GCV
abline(v=0, lty=1) ##OLS
```

```{r}
### RMSPE of `model.0.1.ridge`
library(fastDummies)
data_t_with_dummies <- dummy_cols(data_t, select_columns = "Sex", remove_first_dummy = TRUE)
data_t_with_dummies$Sex <- NULL
data_t_with_dummies <- data_t_with_dummies[, c(9, 10, 1:8)]

data_v_with_dummies <- dummy_cols(data_v, select_columns = "Sex", remove_first_dummy = TRUE)
data_v_with_dummies$Sex <- NULL
data_v_with_dummies <- data_v_with_dummies[, c(9, 10, 1:8)]
```

```{r}
coef.GCV=model.0.1.ridge$coef[, which.min(model.0.1.ridge$GCV)]
X.train=scale(data_t_with_dummies[,1:9], center=model.0.1.ridge$xm, scale=model.0.1.ridge$scales)
Yh.train=X.train%*%coef.GCV+model.0.1.ridge$ym
X.test=scale(data_v_with_dummies[,1:9], center=model.0.1.ridge$xm, scale=model.0.1.ridge$scales)
Yh.test=X.test%*%coef.GCV+model.0.1.ridge$ym

c(sqrt(mean((Yh.train-data_t$Age)^2)), sqrt(mean((Yh.test-data_v$Age)^2)))
```
```{r}
plot(diag(solve(t(X.train)%*%X.train))/diag(solve(t(X.train)%*%X.train+lambda.GCV*diag(9), t(X.train)%*%X.train)%*%solve(t(X.train)%*%X.train+lambda.GCV*diag(9))), ylab="ratio of variance between OLS estimator and ridge estimator", log="y")
abline(h=1, lty=2)
```

### Principle Component Analysis (PCA)

```{r}
par(mfrow=c(1,2))
abapca<-prcomp(data_t_with_dummies[,1:9], center=TRUE, scale=TRUE)
plot(abapca$sdev, type='p', xlab="index", ylab="singular values", log="x", main="Scree Plot ")
plot(cumsum((abapca$sdev)^2)/sum((abapca$sdev)^2), type='p', xlab="index", ylab="cumulative percentage", log="x", main="Cumulative Percentage 
     of Variance Explained by PC")
```
```{r}
cumsum((abapca$sdev)^2)/sum((abapca$sdev)^2)
```

```{r}
par(mfrow=c(1,1))
#plot PC directions/loadings that generate the first five principal components against the X index (frequency index)
matplot(1:9, abapca$rotation[,1:7], type='l', xlab="frequency index", ylab="loading")
legend('topright', legend=1:7, lty=1:7, col=1:7)
```
### Principle Component Regression (PCR)

```{r}
library(pls)  #pcr 
k.max=9   
#fit models with 1,2,.., k.max components successively; 
#standardize X and center Y; conduct "CV" (5 folds, consecutive segments)
#fitted values and coefficients (of the standardized X) of these k.max models are returned;
aba.pcr=pcr(Age~., data=data_t_with_dummies, ncomp=k.max, center=TRUE, scale=TRUE,  validation="CV", segments = 5, segment.type = "consecutive")

validationplot(aba.pcr, main="PCR CV plot")
```
```{r}
### plot fitted coefficients under the original scales
par(mfrow=c(2,2))
plot(coefficients(aba.ols), xlab="frequency index",ylab="fitted coefficient", main="OLS", type='l')
plot(model.0.1.ridge$coef[, which.min(model.0.1.ridge$GCV)]/model.0.1.ridge$scales, xlab="frequency index", ylab="fitted coefficient", main="ridge (with GCV)", type='l')

plot(coef(aba.pcr, ncomp = 7)/aba.pcr$scale,xlab="frequency index", ylab="fitted coefficient", main="PCR: 7 components", type='l')
plot(coef(aba.pcr, ncomp = 9)/aba.pcr$scale, xlab="frequency index", ylab="fitted coefficient", main="PCR: 9 components", type='l')
```
### Comparison between OLS, Ridge, and PCR
```{r}
rmspe<-function(y, yh) sqrt(mean((y-yh)^2))

##OLS 
c(rmspe(data_t$Age, aba.ols$fitted.values),rmspe(data_v$Age, predict(aba.ols, data_v)))
##Ridge
coef.GCV=model.0.1.ridge$coef[, which.min(model.0.1.ridge$GCV)]
X.train=scale(data_t_with_dummies[,1:9], center=model.0.1.ridge$xm, scale=model.0.1.ridge$scales)
Yh.train=X.train%*%coef.GCV+model.0.1.ridge$ym
X.test=scale(data_v_with_dummies[,1:9], center=model.0.1.ridge$xm, scale=model.0.1.ridge$scales)
Yh.test=X.test%*%coef.GCV+model.0.1.ridge$ym
c(rmspe(data_t$Age, Yh.train), rmspe(data_v$Age, Yh.test))
##PCR
c(rmspe(data_t$Age, predict(aba.pcr, ncomp=7)), rmspe(data_v$Age, predict(aba.pcr, data_v_with_dummies, ncomp=7)))
c(rmspe(data_t$Age, predict(aba.pcr, ncomp=9)), rmspe(data_v$Age, predict(aba.pcr, data_v_with_dummies, ncomp=9)))
```
```{r}
### the effective number of X variables in the ridge model with GCV selected lambda is:
lambda.GCV=model.0.1.ridge$lambda[which.min(model.0.1.ridge$GCV)]
S= X.train%*%solve(t(X.train)%*%X.train+lambda.GCV*diag(9), t(X.train)) ## the smoothing matrix 
sum(diag(S)) ## effective number
```
The effective number still close to 9, which means the penalization is very slight


